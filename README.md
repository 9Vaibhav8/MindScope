# MindScope ðŸ§ âœ¨

Encouraging proactive mental health care through AI-powered emotional insights.

MindScope is a multimodal system designed to understand and support mental well-being by analyzing text, voice, and facial expressions. By combining cutting-edge AI with an accessible, digital-first approach, MindScope helps users recognize emotional patterns and take proactive steps toward mental health care.

# Features

# Text Analysis (Core Module)

Model: DistilBERT-base-uncased (lightweight transformer from Hugging Face)
Task: Sentiment and emotion classification on userâ€™s transcribed speech

# Audio Emotion Recognition 

Model: Wav2Vec2-base (Facebook AI) 
Task: Detects emotions from speech using vocal tone, pitch, pace, and energy

# Video Emotion Recognition 

Activation: Enabled only with explicit camera consent
Model: DeepFace or FER+ for facial emotion recognition
Task: Real-time detection of facial expressions

# Tech Stack

NLP: Hugging Face Transformers (DistilBERT)
Audio Processing: Wav2Vec2
Computer Vision: DeepFace, FER+
Frameworks: PyTorch, TensorFlow, OpenCV
Frontend : React.js , Tailwind CSS , Javascript 
Backend : Node.js , Express.js , Mongoose , RestfulAPI


<img width="518" height="299" alt="Screenshot 2025-10-21 211622" src="https://github.com/user-attachments/assets/67b2c090-8d0b-40b9-ab39-5d9943ef9cd5" />
